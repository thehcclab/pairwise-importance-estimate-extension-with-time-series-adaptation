{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9e593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from typing import List\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb96ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_data_path = './EEG/number'\n",
    "processed_data_path = './EEG/processed_data'\n",
    "split_data_path = './EEG/split_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c74d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = os.listdir(participant_data_path)\n",
    "participants=[]\n",
    "for p in ps:\n",
    "    p= p[:3]\n",
    "    if p not in participants:\n",
    "        participants.append(p)\n",
    "        \n",
    "participants= sorted(participants)\n",
    "# participants, len(participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f3009e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_name=f\"userfold_data_scaled_p_dictionary-number\"\n",
    "data_dir = \"./EEG/split_data/standard_scaled\"\n",
    "try:\n",
    "    raw_user_fold= pickle.load(open(os.path.join(data_dir, f\"{load_name}.pkl\"), \"rb\"))\n",
    "except:\n",
    "    print(f\"pickle file does not exist. Use EEG-Preprocess.ipynb and EEG-Split.ipynb to save data setting.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "082a80ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2379064447.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_1814252/2379064447.py\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    from Models.AR_EEG_modelsimport *\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from utilities.userfold_framework import *\n",
    "from Models.AR_EEG_modelsimport *\n",
    "import Models.model_func as Model_Func\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from torcheeg.models import EEGNet\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "DEVICE= torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "learning_rate = 0.00005\n",
    "batch_size = 64\n",
    "n_epochs = 300\n",
    "transpose_channels=True\n",
    "participants_dictionary=[]\n",
    "# participants_online_dictionary=[]\n",
    "participants_grads_dictionary={}\n",
    "b_acc_list=[]\n",
    "c0_acc_list=[]\n",
    "c1_acc_list=[]\n",
    "\n",
    "# EPOCH=[\n",
    "    \n",
    "# ]\n",
    "\n",
    "for i in range(len(participants)):\n",
    "\n",
    "    train_dataloader, val_dataloader, classes, input_dim, class_ratio= user_fold_load(i,\n",
    "                                                                                      raw_user_fold,\n",
    "                                                                                      participants,\n",
    "                                                                                      batch_size=batch_size,\n",
    "                                                                                      transpose_channels=transpose_channels)\n",
    "\n",
    "    classifier= EEGNet(\n",
    "        chunk_size=input_dim[1],\n",
    "        num_electrodes=input_dim[0],\n",
    "        num_classes=classes,\n",
    "        kernel_1= 32,\n",
    "        kernel_2=32,\n",
    "        F1=8,\n",
    "        F2=16,\n",
    "        dropout=0.5\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    \n",
    "    criterion= torch.nn.CrossEntropyLoss(weight=torch.tensor(class_ratio, dtype=torch.float).to(DEVICE))\n",
    "#     criterion = nn.NLLLoss(weight=torch.tensor(class_ratio, dtype=torch.float).to(DEVICE))\n",
    "        \n",
    "    saved_dir= \"./EEG/saved_models/Userfold/run1\"\n",
    "    model = EEGNet_IE_TS_Wrapper(DEVICE, classifier, input_dim[1]).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "#     optimizer= torch.optim.RMSprop(classifier.parameters(), lr=learning_rate)\n",
    "    scheduler= torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "    \n",
    "    train_func= eeg_train\n",
    "    model.training_procedure(iteration=n_epochs,\n",
    "                                    train_dataloader=train_dataloader,\n",
    "                                     val_dataloader=val_dataloader,\n",
    "                                     print_cycle=2,\n",
    "                                     path=f\"./dictionary/intermdiate_dicts\",\n",
    "                                     loss_func=criterion,\n",
    "                                     optimiser=optimizer, #scheduler=scheduler,\n",
    "                                     train_func=train_func\n",
    "                                    )\n",
    "    if model.epoch == n_epochs+1:\n",
    "        EPOCH= n_epochs\n",
    "    else:\n",
    "        EPOCH= model.epoch\n",
    "    \n",
    "    torch.save(model.state_dict(), \n",
    "           os.path.join(\n",
    "               saved_dir, f\"Userfold-{participants[i]}-EEGNet-Weight_TS-e{EPOCH}.pt\"\n",
    "           )\n",
    "    )\n",
    "\n",
    "    pickle.dump( model.return_IE_weights(), \n",
    "                open(f\"{saved_dir}/Userfold-{participants[i]}-EEGNet-Weight_TS-w-e{EPOCH}.pkl\", \"wb\") \n",
    "               )    \n",
    "\n",
    "# OR\n",
    "#     model.load_state_dict(\n",
    "#     torch.load(\n",
    "#         open(\n",
    "#             os.path.join(\n",
    "#                 saved_dir, f\"Userfold-{participants[i]}-EEGNet-Weight_TS-e{n_epochs}.pt\"\n",
    "#             ), \"rb\"\n",
    "#         )\n",
    "#               )\n",
    "#     )\n",
    "#     pickle.load( \n",
    "#                 open(f\"{saved_dir}/Userfold-{participants[i]}-EEGNet-Weight_TS-w-e{EPOCH}.pkl\", \"rb\") \n",
    "#                )  \n",
    "    \n",
    "    prediction, dictionary= model.prediction_procedure(val_dataloader, dict_flag=True)\n",
    "    \n",
    "    ys= np.concatenate([y.detach().cpu().numpy() for x, y in val_dataloader])\n",
    "    \n",
    "    c0_acc, c1_acc, b_acc= calculate_accuracy(ys, prediction)\n",
    "    print(\"c0_acc\", c0_acc, \", c1_acc\", c1_acc, \", b_acc\", b_acc)\n",
    "    b_acc_list.append(b_acc)\n",
    "    c0_acc_list.append(c0_acc)\n",
    "    c1_acc_list.append(c1_acc)\n",
    "    participants_dictionary.append(dictionary)\n",
    "    \n",
    "\n",
    "tmp=[]\n",
    "for i, dictionary in enumerate(participants_dictionary):\n",
    "    print(f\"User {participants[i]} f1: {dictionary['weighted avg']['f1-score']} acc: {dictionary['accuracy']}\")\n",
    "    print(f\" c0: {c0_acc_list[i]} c1: {c1_acc_list[i]} bacc: {b_acc_list[i]}\")\n",
    "    tmp.append(dictionary['weighted avg']['f1-score'])\n",
    "\n",
    "print(f\"average {np.mean(tmp)}\")\n",
    "print()\n",
    "print(np.array(b_acc_list).mean())\n",
    "print(np.array(c1_acc_list).mean())\n",
    "print(np.array(c0_acc_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9de653",
   "metadata": {},
   "outputs": [],
   "source": [
    "userfold_results_summary(participants_dictionary, participants)\n",
    "userfold_classwise_results_summary(participants_dictionary, participants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.return_IE_weights().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53801378",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=-0.5\n",
    "step=1.5/(188-1)\n",
    "timestep_labels=[]\n",
    "for i in range(input_dim[1]):\n",
    "    timestep_labels.append(round(start+step*i,3))\n",
    "    \n",
    "# timestep_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa03a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "participants_w_list=[]\n",
    "\n",
    "for i in range(len(participants)):\n",
    "\n",
    "    w= pickle.load(\n",
    "        open(f\"{saved_dir}/Userfold-{participants[i]}-EEGNet-Weight_TS-w-e{EPOCH}.pkl\", \"rb\") \n",
    "                    )  \n",
    "    participants_w_list.append(w)\n",
    "    \n",
    "avg_w= np.array(participants_w_list).mean(axis=0)\n",
    "# scaler= MinMaxScaler()\n",
    "# scaled_avg_w= scaler.fit_transform(avg_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "\n",
    "scaler= MinMaxScaler()\n",
    "scaled_avg_w= scaler.fit_transform(avg_w.reshape(-1,1))\n",
    "df= pd.DataFrame(scaled_avg_w)\n",
    "# df.index= channel_names\n",
    "# df.columns=timestep_labels\n",
    "\n",
    "\n",
    "plt.plot(df)\n",
    "# plt.legend(methods)\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.axvspan(107,121, color=\"green\", alpha=0.2)\n",
    "plt.axvspan(112,128, color=\"red\", alpha=0.2)\n",
    "plt.axvspan(142,150, color=\"red\", alpha=0.2)\n",
    "plt.axvspan(157,159, color=\"red\", alpha=0.2)\n",
    "plt.xticks([0,31,62,93,124,155,187],[-0.5, -0.25, 0, 0.25, 0.5, 0.75, 1.0])\n",
    "plt.ylabel(\"Normalised Importance\")\n",
    "plt.margins(x=0)\n",
    "matplotlib.rcParams.update({\"font.size\":18})\n",
    "plt.tight_layout()\n",
    "#     sns.heatmap(df.sum().to_numpy().reshape(-1,1),annot=True, \n",
    "#                 yticklabels=timestep_labels, ax=ax[i][1],\n",
    "#                 xticklabels=False, cbar_kws={\"pad\":0.02})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
