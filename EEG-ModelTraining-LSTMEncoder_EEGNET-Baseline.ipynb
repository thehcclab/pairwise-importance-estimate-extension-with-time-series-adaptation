{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c9e593e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from typing import List\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb96ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "participant_data_path = './EEG/number'\n",
    "processed_data_path = './EEG/processed_data'\n",
    "split_data_path = './EEG/split_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c74d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = os.listdir(participant_data_path)\n",
    "participants=[]\n",
    "for p in ps:\n",
    "    p= p[:3]\n",
    "    if p not in participants:\n",
    "        participants.append(p)\n",
    "        \n",
    "participants= sorted(participants)\n",
    "# participants, len(participants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28f3009e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "load_name=f\"userfold_data_scaled_p_dictionary-number\"\n",
    "data_dir = \"./EEG/split_data/standard_scaled\"\n",
    "try:\n",
    "    raw_user_fold= pickle.load(open(os.path.join(data_dir, f\"{load_name}.pkl\"), \"rb\"))\n",
    "except:\n",
    "    print(f\"pickle file does not exist. Use EEG-Preprocess.ipynb and EEG-Split.ipynb to save data setting.\")\n",
    "    sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082a80ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utilities.userfold_framework import *\n",
    "# from utilities.EEG_func import *\n",
    "from Models.AR_EEG_models import *\n",
    "import Models.model_func as Model_Func\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from torcheeg.models import EEGNet\n",
    "# import Models.model_func as Model_Func\n",
    "from torch import nn\n",
    "from Models.multi_models import *\n",
    "\n",
    "DEVICE= torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "learning_rate = 0.00005\n",
    "batch_size = 64\n",
    "n_epochs = 300\n",
    "transpose_channels=True\n",
    "participants_dictionary=[]\n",
    "# participants_online_dictionary=[]\n",
    "participants_grads_dictionary={}\n",
    "b_acc_list=[]\n",
    "c0_acc_list=[]\n",
    "c1_acc_list=[]\n",
    "\n",
    "# EPOCH=[\n",
    "    \n",
    "# ]\n",
    "\n",
    "for i in range(len(participants)):\n",
    "\n",
    "    train_dataloader, val_dataloader, classes, input_dim, class_ratio= user_fold_load(i,\n",
    "                                                                                      raw_user_fold,\n",
    "                                                                                      participants,\n",
    "                                                                                      batch_size=batch_size,\n",
    "                                                                                      transpose_channels=transpose_channels)\n",
    "\n",
    "    classifier= EEGNet(\n",
    "        chunk_size=input_dim[1],\n",
    "        num_electrodes=input_dim[0],\n",
    "        num_classes=classes,\n",
    "        kernel_1= 32,\n",
    "        kernel_2=32,\n",
    "        F1=8,\n",
    "        F2=16,\n",
    "        dropout=0.5\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "#     resnet = ResNetPlus(input_dim[0], classes, bn_1st=False)\n",
    "#     softmax_activation = nn.LogSoftmax(dim=1)\n",
    "#     classifier = nn.Sequential(resnet, softmax_activation).to(DEVICE)\n",
    "\n",
    "    \n",
    "#     classifier = DataGliderBasic_Model(DEVICE, input_dim, classes)\n",
    "#     classifier.to(DEVICE)\n",
    "    \n",
    "#     optimizer= torch.optim.RMSprop(classifier.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "    criterion= torch.nn.CrossEntropyLoss(weight=torch.tensor(class_ratio, dtype=torch.float).to(DEVICE))\n",
    "#     criterion = nn.NLLLoss(weight=torch.tensor(class_ratio, dtype=torch.float).to(DEVICE))\n",
    "    \n",
    "    \n",
    "    saved_dir= \"./EEG/saved_models/Userfold/run2\"\n",
    "#     model= Multivariate_IEGradient(DEVICE, input_dim, resnet_classifier).to(DEVICE)\n",
    "    model= LSTMEncoder_EEGNet_Wrapper(DEVICE, classifier, input_dim).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "    scheduler= torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "    train_func= eeg_train\n",
    "    model.training_procedure(iteration=n_epochs,\n",
    "                                    train_dataloader=train_dataloader,\n",
    "                                     val_dataloader=val_dataloader,\n",
    "                                     print_cycle=2,\n",
    "                                     path=f\"./dictionary/intermdiate_dicts\",\n",
    "                                     loss_func=criterion,\n",
    "                                     optimiser=optimizer, #scheduler=scheduler,\n",
    "                                     train_func=train_func\n",
    "                                    )\n",
    "    if model.epoch == n_epochs+1:\n",
    "        EPOCH= n_epochs\n",
    "    else:\n",
    "        EPOCH= model.epoch\n",
    "    \n",
    "    torch.save(model.state_dict(), \n",
    "           os.path.join(\n",
    "               saved_dir, f\"Userfold-{participants[i]}-LSTMEncoder_EEGNet-Baseline-e{EPOCH}.pt\"\n",
    "           )\n",
    "    )\n",
    "\n",
    "# OR\n",
    "#     model.load_state_dict(\n",
    "#     torch.load(\n",
    "#         open(\n",
    "#             os.path.join(\n",
    "#                 saved_dir, f\"Userfold-{participants[i]}-LSTMEncoder_EEGNet-Baseline-e{n_epochs}.pt\"\n",
    "#             ), \"rb\"\n",
    "#         )\n",
    "#               )\n",
    "#     )\n",
    "\n",
    "    \n",
    "    prediction, dictionary= model.prediction_procedure(val_dataloader, dict_flag=True)\n",
    "    \n",
    "    ys= np.concatenate([y.detach().cpu().numpy() for x, y in val_dataloader])\n",
    "    \n",
    "    c0_acc, c1_acc, b_acc= calculate_accuracy(ys, prediction)\n",
    "    print(\"c0_acc\", c0_acc, \", c1_acc\", c1_acc, \", b_acc\", b_acc)\n",
    "    b_acc_list.append(b_acc)\n",
    "    c0_acc_list.append(c0_acc)\n",
    "    c1_acc_list.append(c1_acc)\n",
    "    participants_dictionary.append(dictionary)\n",
    "    \n",
    "\n",
    "tmp=[]\n",
    "for i, dictionary in enumerate(participants_dictionary):\n",
    "    print(f\"User {participants[i]} f1: {dictionary['weighted avg']['f1-score']} acc: {dictionary['accuracy']}\")\n",
    "    print(f\" c0: {c0_acc_list[i]} c1: {c1_acc_list[i]} bacc: {b_acc_list[i]}\")\n",
    "    tmp.append(dictionary['weighted avg']['f1-score'])\n",
    "\n",
    "print(f\"average {np.mean(tmp)}\")\n",
    "print()\n",
    "print(np.array(b_acc_list).mean())\n",
    "print(np.array(c1_acc_list).mean())\n",
    "print(np.array(c0_acc_list).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9de653",
   "metadata": {},
   "outputs": [],
   "source": [
    "userfold_results_summary(participants_dictionary, participants)\n",
    "userfold_classwise_results_summary(participants_dictionary, participants)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
